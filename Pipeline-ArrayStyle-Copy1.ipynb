{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cf = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDataFrame(data_path):\n",
    "    \"\"\"\n",
    "    input: data_path: path to data\n",
    "    return: data frame\n",
    "    \"\"\"\n",
    "    data_frame = pd.read_csv(data_path)\n",
    "    data_frame.columns = ['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createTrainingDataPathsCenterLeftRightPrefixGood(df, start, end, correction_factor, prefix_path):\n",
    "    \"\"\"\n",
    "    creates training data and training labels/ measurements from a data frame\n",
    "    inputs:\n",
    "    df: pandas DataFrame object\n",
    "    start: starting row to grab data\n",
    "    end: ending row to grab data\n",
    "    correction_factor: factor to correct steering angles\n",
    "    \"\"\"\n",
    "    center_images = []\n",
    "    left_images = []\n",
    "    right_images = []\n",
    "    measurements = []\n",
    "    abs_path_to_IMG = os.path.abspath(prefix_path)\n",
    "    for idx, row in df.iterrows():\n",
    "        if start <= idx <= end:\n",
    "            # center images\n",
    "            center_images.append(os.path.join(abs_path_to_IMG, row['center'].strip()))\n",
    "            measurements.append(row['steering'])\n",
    "\n",
    "            # left images\n",
    "            left_images.append(os.path.join(abs_path_to_IMG, row['left'].strip()))\n",
    "            measurements.append(row['steering'] - correction_factor)\n",
    "\n",
    "            # right images\n",
    "            right_images.append(os.path.join(abs_path_to_IMG, row['right'].strip()))\n",
    "            measurements.append(row['steering'] + correction_factor)\n",
    "            \n",
    "    return (np.asarray(center_images), np.asarray(left_images), np.asarray(right_images), np.asarray(measurements, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createTrainingDataPathsCLR(df, start, end, correction_factor, prefix_path):\n",
    "    \"\"\"\n",
    "    creates training data and training labels/ measurements from a data frame\n",
    "    inputs:\n",
    "    df: pandas DataFrame object\n",
    "    start: starting row to grab data\n",
    "    end: ending row to grab data\n",
    "    correction_factor: factor to correct steering angles\n",
    "    \"\"\"\n",
    "    center_images = []\n",
    "    left_images = []\n",
    "    right_images = []\n",
    "    \n",
    "    center_measurements = []\n",
    "    left_measurements = []\n",
    "    right_measurements = []\n",
    "    \n",
    "    abs_path_to_IMG = os.path.abspath(prefix_path)\n",
    "    for idx, row in df.iterrows():\n",
    "        if start <= idx <= end:\n",
    "            # center images\n",
    "            center_images.append(os.path.join(abs_path_to_IMG, row['center'].strip()))\n",
    "            center_measurements.append(row['steering'])\n",
    "\n",
    "            # left images\n",
    "            left_images.append(os.path.join(abs_path_to_IMG, row['left'].strip()))\n",
    "            left_measurements.append(row['steering'] - correction_factor)\n",
    "\n",
    "            # right images\n",
    "            right_images.append(os.path.join(abs_path_to_IMG, row['right'].strip()))\n",
    "            right_measurements.append(row['steering'] + correction_factor)\n",
    "            \n",
    "    return (\n",
    "        np.asarray(center_images),\n",
    "        np.asarray(left_images),\n",
    "        np.asarray(right_images),\n",
    "        np.asarray(center_measurements, dtype=np.float32),\n",
    "        np.asarray(left_measurements, dtype=np.float32),\n",
    "        np.asarray(right_measurements, dtype=np.float32),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeZerosFromCenter(X, y):\n",
    "    nonzero = np.nonzero(y)\n",
    "    y = y[nonzero]\n",
    "    X = X[nonzero]    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeZerosFromLeft(X, y, correction_factor, rang = 0.001):\n",
    "    low = -1 * (correction_factor + rang)\n",
    "    high = -1 * (correction_factor - rang)\n",
    "    is_y_in_range = np.logical_or(y <= low, y >= high)\n",
    "    X = X[is_y_in_range]\n",
    "    y = y[is_y_in_range]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeZerosFromRight(X, y, correction_factor, rang = 0.001):\n",
    "    high = correction_factor + rang\n",
    "    low = correction_factor - rang\n",
    "    is_y_range = np.logical_or(y <= low, y >= high)\n",
    "    X = X[is_y_range]\n",
    "    y = y[is_y_range]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data from dirt recovery (ccw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_pd_dirt_c1 = createDataFrame('A_Recovery_right_lane_dirt/driving_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train_dirt_c1_c, \n",
    "X_train_dirt_c1_l, \n",
    "X_train_dirt_c1_r, \n",
    "y_train_dirt_c1_c,\n",
    "y_train_dirt_c1_l,\n",
    "y_train_dirt_c1_r) = createTrainingDataPathsCLR(data_pd_dirt_c1, 0, 300, cf, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_dirt_c1_c shape:  (301,)\n",
      "y_train_dirt_c1_c shape:  (301,)\n",
      "y_train_dirt_c1_l shape:  (301,)\n",
      "X_train_dirt_c1_l shape:  (301,)\n",
      "X_train_dirt_c1_r shape:  (301,)\n",
      "y_train_dirt_c1_r shape:  (301,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_dirt_c1_c shape: ', X_train_dirt_c1_c.shape)\n",
    "print('y_train_dirt_c1_c shape: ', y_train_dirt_c1_c.shape)\n",
    "print('y_train_dirt_c1_l shape: ', X_train_dirt_c1_l.shape)\n",
    "print('X_train_dirt_c1_l shape: ', y_train_dirt_c1_l.shape)\n",
    "print('X_train_dirt_c1_r shape: ', X_train_dirt_c1_r.shape)\n",
    "print('y_train_dirt_c1_r shape: ', y_train_dirt_c1_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove steering data with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15\n"
     ]
    }
   ],
   "source": [
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_dirt_c1_c, y_train_dirt_c1_c = removeZerosFromCenter(X_train_dirt_c1_c, y_train_dirt_c1_c)\n",
    "X_train_dirt_c1_l, y_train_dirt_c1_l = removeZerosFromLeft(X_train_dirt_c1_l, y_train_dirt_c1_l, cf)\n",
    "X_train_dirt_c1_r, y_train_dirt_c1_r = removeZerosFromRight(X_train_dirt_c1_r, y_train_dirt_c1_r, cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_dirt_c1_c shape:  (239,)\n",
      "y_train_dirt_c1_c shape:  (239,)\n",
      "y_train_dirt_c1_l shape:  (239,)\n",
      "X_train_dirt_c1_l shape:  (239,)\n",
      "X_train_dirt_c1_r shape:  (239,)\n",
      "y_train_dirt_c1_r shape:  (239,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_dirt_c1_c shape: ', X_train_dirt_c1_c.shape)\n",
    "print('y_train_dirt_c1_c shape: ', y_train_dirt_c1_c.shape)\n",
    "print('y_train_dirt_c1_l shape: ', X_train_dirt_c1_l.shape)\n",
    "print('X_train_dirt_c1_l shape: ', y_train_dirt_c1_l.shape)\n",
    "print('X_train_dirt_c1_r shape: ', X_train_dirt_c1_r.shape)\n",
    "print('y_train_dirt_c1_r shape: ', y_train_dirt_c1_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_dirt_c1 shape:  (717,)\n",
      "y_train_dirt_c1 shape:  (717,)\n"
     ]
    }
   ],
   "source": [
    "X_train_dirt_c1 = np.append(X_train_dirt_c1_c, X_train_dirt_c1_l, axis = 0)\n",
    "X_train_dirt_c1 = np.append(X_train_dirt_c1, X_train_dirt_c1_r, axis = 0)\n",
    "print('X_train_dirt_c1 shape: ', X_train_dirt_c1.shape)\n",
    "y_train_dirt_c1 = np.append(y_train_dirt_c1_c, y_train_dirt_c1_l, axis = 0)\n",
    "y_train_dirt_c1 = np.append(y_train_dirt_c1, y_train_dirt_c1_r, axis = 0)\n",
    "print('y_train_dirt_c1 shape: ', y_train_dirt_c1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>=> X_train_dirt_c1, y_train_dirt_c1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data from recovery_by_lake_1 (cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pd_lake_c1 = createDataFrame('recovery_cw_bylake/driving_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train_lake_c1_c, \n",
    "X_train_lake_c1_l, \n",
    "X_train_lake_c1_r, \n",
    "y_train_lake_c1_c,\n",
    "y_train_lake_c1_l,\n",
    "y_train_lake_c1_r) = createTrainingDataPathsCLR(data_pd_lake_c1, 1, 550, cf, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove steering data with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lake_c1_c, y_train_lake_c1_c = removeZerosFromCenter(X_train_lake_c1_c, y_train_lake_c1_c)\n",
    "X_train_lake_c1_l, y_train_lake_c1_l = removeZerosFromLeft(X_train_lake_c1_l, y_train_lake_c1_l, cf)\n",
    "X_train_lake_c1_r, y_train_lake_c1_r = removeZerosFromRight(X_train_lake_c1_r, y_train_lake_c1_r, cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lake_c1_c shape:  (454,)\n",
      "y_train_lake_c1_c shape:  (454,)\n",
      "y_train_lake_c1_l shape:  (454,)\n",
      "X_train_lake_c1_l shape:  (454,)\n",
      "X_train_lake_c1_r shape:  (454,)\n",
      "y_train_lake_c1_r shape:  (454,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_lake_c1_c shape: ', X_train_lake_c1_c.shape)\n",
    "print('y_train_lake_c1_c shape: ', y_train_lake_c1_c.shape)\n",
    "print('y_train_lake_c1_l shape: ', X_train_lake_c1_l.shape)\n",
    "print('X_train_lake_c1_l shape: ', y_train_lake_c1_l.shape)\n",
    "print('X_train_lake_c1_r shape: ', X_train_lake_c1_r.shape)\n",
    "print('y_train_lake_c1_r shape: ', y_train_lake_c1_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lake_c1 shape:  (1362,)\n",
      "y_train_lake_c1 shape:  (1362,)\n"
     ]
    }
   ],
   "source": [
    "X_train_lake_c1 = np.append(X_train_lake_c1_c, X_train_lake_c1_l, axis = 0)\n",
    "X_train_lake_c1 = np.append(X_train_lake_c1, X_train_lake_c1_r, axis = 0)\n",
    "print('X_train_lake_c1 shape: ', X_train_lake_c1.shape)\n",
    "y_train_lake_c1 = np.append(y_train_lake_c1_c, y_train_lake_c1_l, axis = 0)\n",
    "y_train_lake_c1 = np.append(y_train_lake_c1, y_train_lake_c1_r, axis = 0)\n",
    "print('y_train_lake_c1 shape: ', y_train_lake_c1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>=> X_train_lake_c1, y_train_lake_c1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data from recovery_by_lake_cw_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pd_lake_c2 = createDataFrame('recovery_cw_by_lake_2/driving_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train_lake_c2_c, \n",
    "X_train_lake_c2_l, \n",
    "X_train_lake_c2_r, \n",
    "y_train_lake_c2_c,\n",
    "y_train_lake_c2_l,\n",
    "y_train_lake_c2_r) = createTrainingDataPathsCLR(data_pd_lake_c2, 1, 125, cf, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove steering data with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_lake_c2_c, y_train_lake_c2_c = removeZerosFromCenter(X_train_lake_c2_c, y_train_lake_c2_c)\n",
    "X_train_lake_c2_l, y_train_lake_c2_l = removeZerosFromLeft(X_train_lake_c2_l, y_train_lake_c2_l, cf)\n",
    "X_train_lake_c2_r, y_train_lake_c2_r = removeZerosFromRight(X_train_lake_c2_r, y_train_lake_c2_r, cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lake_c2_c shape:  (122,)\n",
      "y_train_lake_c2_c shape:  (122,)\n",
      "y_train_lake_c2_l shape:  (122,)\n",
      "X_train_lake_c2_l shape:  (122,)\n",
      "X_train_lake_c2_r shape:  (122,)\n",
      "y_train_lake_c2_r shape:  (122,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_lake_c2_c shape: ', X_train_lake_c2_c.shape)\n",
    "print('y_train_lake_c2_c shape: ', y_train_lake_c2_c.shape)\n",
    "print('y_train_lake_c2_l shape: ', X_train_lake_c2_l.shape)\n",
    "print('X_train_lake_c2_l shape: ', y_train_lake_c2_l.shape)\n",
    "print('X_train_lake_c2_r shape: ', X_train_lake_c2_r.shape)\n",
    "print('y_train_lake_c2_r shape: ', y_train_lake_c2_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lake_c2 shape:  (366,)\n",
      "y_train_lake_c2 shape:  (366,)\n"
     ]
    }
   ],
   "source": [
    "X_train_lake_c2 = np.append(X_train_lake_c2_c, X_train_lake_c2_l, axis = 0)\n",
    "X_train_lake_c2 = np.append(X_train_lake_c2, X_train_lake_c2_r, axis = 0)\n",
    "print('X_train_lake_c2 shape: ', X_train_lake_c2.shape)\n",
    "y_train_lake_c2 = np.append(y_train_lake_c2_c, y_train_lake_c2_l, axis = 0)\n",
    "y_train_lake_c2 = np.append(y_train_lake_c2, y_train_lake_c2_r, axis = 0)\n",
    "print('y_train_lake_c2 shape: ', y_train_lake_c2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>=> X_train_lake_c2, y_train_lake_c2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data from udacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_udacity:  (23958,)\n",
      "shape of y_train_udacity:  (23958,)\n"
     ]
    }
   ],
   "source": [
    "data_pd_cw2 = createDataFrame('data/driving_log.csv')\n",
    "X_train_cw2_c, X_train_cw2_l, X_train_cw2_r, y_train_cw2 = createTrainingDataPathsCenterLeftRightPrefixGood(data_pd_cw2, 50, 8036, cf, 'data/')\n",
    "# data_pd_cw2.hist(column = 'throttle')\n",
    "X_train_udacity = np.append(X_train_cw2_c, X_train_cw2_l, axis = 0)\n",
    "X_train_udacity = np.append(X_train_udacity, X_train_cw2_r, axis = 0)\n",
    "y_train_udacity = y_train_cw2\n",
    "print('shape of X_train_udacity: ', X_train_udacity.shape)\n",
    "print('shape of y_train_udacity: ', y_train_udacity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data from annie recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train_annie:  (4038,)\n",
      "shape of y_train_annie:  (4038,)\n"
     ]
    }
   ],
   "source": [
    "data_pd_cwr2 = createDataFrame('annie_recovery/driving_log_recovery.csv')\n",
    "X_train_cwr2_c, X_train_cwr2_l, X_train_cwr2_r, y_train_cwr2 = createTrainingDataPathsCenterLeftRightPrefixGood(data_pd_cwr2, 1, 1348, cf, 'annie_recovery/')\n",
    "# plt.hist(y_train_cwr2,bins=100,range=(-1,1),facecolor=\"r\", histtype = 'step')\n",
    "X_train_annie = np.append(X_train_cwr2_c, X_train_cwr2_l, axis = 0)\n",
    "X_train_annie = np.append(X_train_annie, X_train_cwr2_r, axis = 0)\n",
    "y_train_annie = y_train_cwr2\n",
    "print('shape of X_train_annie: ', X_train_annie.shape)\n",
    "print('shape of y_train_annie: ', y_train_annie.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To demonstrate acquisition function in other file and to check its correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.append(X_train_udacity, X_train_annie, axis = 0)\n",
    "y_train = np.append(y_train_udacity, y_train_annie, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in recovery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.append(X_train, X_train_dirt_c1, axis = 0)\n",
    "X_train = np.append(X_train, X_train_lake_c1, axis = 0)\n",
    "X_train = np.append(X_train, X_train_lake_c2, axis = 0)\n",
    "\n",
    "y_train = np.append(y_train, y_train_dirt_c1, axis = 0)\n",
    "y_train = np.append(y_train, y_train_lake_c1, axis = 0)\n",
    "y_train = np.append(y_train, y_train_lake_c2, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train:  (24352,)\n",
      "shape of y_train:  (24352,)\n",
      "shape of X_valid:  (6089,)\n",
      "shape of y_valid:  (6089,)\n"
     ]
    }
   ],
   "source": [
    "print('shape of X_train: ', X_train.shape)\n",
    "print('shape of y_train: ', y_train.shape)\n",
    "print('shape of X_valid: ', X_valid.shape)\n",
    "print('shape of y_valid: ', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see, most of our steering angles are around zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Preprocess image, \n",
    "    input: image (original shape)\n",
    "    output: image (shape is (220, 66, 3) )\n",
    "    \"\"\"    \n",
    "    # crop shape\n",
    "    image = image[image.shape[0] * 0.34:image.shape[0] * 0.875,:,:]\n",
    "    # resize to (66, 220)\n",
    "    img = cv2.resize(image, (220, 66), interpolation=cv2.INTER_AREA)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image_valid_from_path(image_path, steering_angle):\n",
    "    img = mpimg.imread(image_path)\n",
    "    img = preprocess_image(img)\n",
    "    return img, steering_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image_from_path(image_path, steering_angle):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "    img = preprocess_image(img)\n",
    "    return img, steering_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I created two generators, one for training data and one for validation data. In the training generator we create batches of 32 for each training sample. Therefore if I have `samples_per_epoch = 10` that means for each of those samples I yield 32 images from the training generator. I did this because it allows me to have control over what the batches turn out to be. In this case [ADD HERE]```add here I will add a probability function to pass a certain image path to the processor```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_training_data(X_train, y_train, batch_size):\n",
    "    \"\"\"\n",
    "    We create a loop through out data and \n",
    "    send out an individual row in the dataframe to preprocess_image_from_path, \n",
    "    which is then sent to preprocess_image\n",
    "    inputs: \n",
    "    X_train: numpy array of X_training data (image paths),\n",
    "    y_train: numpy array of measurements (steering angles)\n",
    "    batch_size: batch sizes, size to make each batch\n",
    "    returns a yield (image_batch, label_batch)\n",
    "    \"\"\"\n",
    "    image_batch = np.zeros((batch_size * 2, 66, 220, 3)) # nvidia input params\n",
    "    label_batch = np.zeros((batch_size * 2))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            idx = np.random.randint(len(X_train))\n",
    "            x, y = preprocess_image_from_path(X_train[idx], y_train[idx])\n",
    "            \n",
    "             # flip an image and return it\n",
    "            x2 = np.fliplr(x)\n",
    "            y2 = -1 * y\n",
    "            \n",
    "            image_batch[i] = x\n",
    "            label_batch[i] = y\n",
    "            \n",
    "            image_batch[i+1] = x2\n",
    "            image_batch[i+1] = y2\n",
    "        yield image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_validation_data(X_valid, y_valid):\n",
    "    while True:\n",
    "        for idx in range(len(X_valid)):\n",
    "            img, angle = preprocess_image_valid_from_path(X_valid[idx], y_valid[idx])\n",
    "            img = img.reshape(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "            angle = np.array([[angle]])\n",
    "            yield img, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I chose to use Nvidia's network architecture. Input (220 x 66 sized image) output (1 steering angle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use the Nvidia model architecture which can be found [here add link]\n",
    "I used ELu's because they push mean unit activation functions closer to zero [https://arxiv.org/pdf/1511.07289v1.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense, Lambda\n",
    "from keras.layers import ELU\n",
    "from keras.optimizers import Adam\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "\n",
    "N_img_height = 66\n",
    "N_img_width = 220\n",
    "N_img_channels = 3\n",
    "def nvidia_model():\n",
    "    inputShape = (N_img_height, N_img_width, N_img_channels)\n",
    "\n",
    "    model = Sequential()\n",
    "    # normalization\n",
    "    model.add(Lambda(lambda x: x / 127.5 - 1, input_shape = (66, 220, 3)))\n",
    "    # cropping 70 off top 25 off bottom\n",
    "    # model.add(Cropping2D(cropping=((70,25), (0, 0)))) Probably going to do cropping in my process\n",
    "\n",
    "    # subsample is strides\n",
    "    model.add(Convolution2D(24, 5, 5, \n",
    "                            subsample=(2,2), \n",
    "                            border_mode = 'valid',\n",
    "                            init = 'he_normal',\n",
    "                            name = 'conv1'))\n",
    "    \n",
    "    model.add(ELU())    \n",
    "    model.add(Convolution2D(36, 5, 5, \n",
    "                            subsample=(2,2), \n",
    "                            border_mode = 'valid',\n",
    "                            init = 'he_normal',\n",
    "                            name = 'conv2'))\n",
    "    \n",
    "    model.add(ELU())    \n",
    "    model.add(Convolution2D(48, 5, 5, \n",
    "                            subsample=(2,2), \n",
    "                            border_mode = 'valid',\n",
    "                            init = 'he_normal',\n",
    "                            name = 'conv3'))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(64, 3, 3, \n",
    "                            subsample = (1,1), \n",
    "                            border_mode = 'valid',\n",
    "                            init = 'he_normal', #gaussian init\n",
    "                            name = 'conv4'))\n",
    "    \n",
    "    model.add(ELU())              \n",
    "    model.add(Convolution2D(64, 3, 3, \n",
    "                            subsample= (1,1), \n",
    "                            border_mode = 'valid',\n",
    "                            init = 'he_normal',\n",
    "                            name = 'conv5'))\n",
    "              \n",
    "              \n",
    "    model.add(Flatten(name = 'flatten'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(100, init = 'he_normal', name = 'fc1'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(50, init = 'he_normal', name = 'fc2'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(10, init = 'he_normal', name = 'fc3'))\n",
    "    model.add(ELU())\n",
    "    \n",
    "    # do not put activation at the end because we want to exact output, not a class identifier\n",
    "    model.add(Dense(1, name = 'output', init = 'he_normal'))\n",
    "    \n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer = adam, loss = 'mse')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_size = len(X_valid)\n",
    "valid_generator = generate_validation_data(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/anaconda2/envs/python3/lib/python3.5/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2048/24352 [=>............................] - ETA: 34s - loss: 0.0329"
     ]
    }
   ],
   "source": [
    "model = nvidia_model()\n",
    "for i in range(3):\n",
    "    train_generator = generate_training_data(X_train, y_train, 128)\n",
    "    history = model.fit_generator(\n",
    "            train_generator, \n",
    "            samples_per_epoch = len(X_train), # try putting the whole thing in here in the future\n",
    "            nb_epoch = 6,\n",
    "            validation_data = valid_generator,\n",
    "            nb_val_samples = val_size)\n",
    "    print(history)\n",
    "    \n",
    "    model.save_weights('model-weightsA2.h5')\n",
    "    model.save('modelA2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "### plot the training and validation loss for each epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
